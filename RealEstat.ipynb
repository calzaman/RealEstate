{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1022 samples, validate on 219 samples\n",
      "Epoch 1/100\n",
      "1022/1022 [==============================] - 0s 111us/step - loss: 0.7070 - accuracy: 0.4824 - val_loss: 0.7077 - val_accuracy: 0.4201\n",
      "Epoch 2/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.6974 - accuracy: 0.4697 - val_loss: 0.6953 - val_accuracy: 0.4475\n",
      "Epoch 3/100\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.6907 - accuracy: 0.5519 - val_loss: 0.6859 - val_accuracy: 0.6256\n",
      "Epoch 4/100\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.6852 - accuracy: 0.6517 - val_loss: 0.6786 - val_accuracy: 0.7169\n",
      "Epoch 5/100\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.6804 - accuracy: 0.6693 - val_loss: 0.6725 - val_accuracy: 0.7397\n",
      "Epoch 6/100\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.6759 - accuracy: 0.7025 - val_loss: 0.6668 - val_accuracy: 0.7671\n",
      "Epoch 7/100\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.6716 - accuracy: 0.7241 - val_loss: 0.6614 - val_accuracy: 0.7717\n",
      "Epoch 8/100\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.6676 - accuracy: 0.7143 - val_loss: 0.6566 - val_accuracy: 0.7854\n",
      "Epoch 9/100\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.6635 - accuracy: 0.7290 - val_loss: 0.6518 - val_accuracy: 0.7854\n",
      "Epoch 10/100\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.6593 - accuracy: 0.7466 - val_loss: 0.6469 - val_accuracy: 0.7991\n",
      "Epoch 11/100\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.6550 - accuracy: 0.7524 - val_loss: 0.6419 - val_accuracy: 0.7945\n",
      "Epoch 12/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.6505 - accuracy: 0.7671 - val_loss: 0.6368 - val_accuracy: 0.8037\n",
      "Epoch 13/100\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.6460 - accuracy: 0.7789 - val_loss: 0.6313 - val_accuracy: 0.8219\n",
      "Epoch 14/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.6412 - accuracy: 0.7877 - val_loss: 0.6252 - val_accuracy: 0.8311\n",
      "Epoch 15/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.6364 - accuracy: 0.7867 - val_loss: 0.6195 - val_accuracy: 0.8356\n",
      "Epoch 16/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.6309 - accuracy: 0.7935 - val_loss: 0.6132 - val_accuracy: 0.8356\n",
      "Epoch 17/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.6255 - accuracy: 0.7926 - val_loss: 0.6067 - val_accuracy: 0.8356\n",
      "Epoch 18/100\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.6198 - accuracy: 0.7955 - val_loss: 0.6001 - val_accuracy: 0.8402\n",
      "Epoch 19/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.6137 - accuracy: 0.8063 - val_loss: 0.5928 - val_accuracy: 0.8493\n",
      "Epoch 20/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.6073 - accuracy: 0.8209 - val_loss: 0.5848 - val_accuracy: 0.8493\n",
      "Epoch 21/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.6009 - accuracy: 0.8102 - val_loss: 0.5784 - val_accuracy: 0.8584\n",
      "Epoch 22/100\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.5942 - accuracy: 0.8219 - val_loss: 0.5710 - val_accuracy: 0.8721\n",
      "Epoch 23/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.5874 - accuracy: 0.8219 - val_loss: 0.5639 - val_accuracy: 0.8721\n",
      "Epoch 24/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.5805 - accuracy: 0.8317 - val_loss: 0.5555 - val_accuracy: 0.8767\n",
      "Epoch 25/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.5732 - accuracy: 0.8288 - val_loss: 0.5477 - val_accuracy: 0.8858\n",
      "Epoch 26/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.5659 - accuracy: 0.8366 - val_loss: 0.5394 - val_accuracy: 0.8858\n",
      "Epoch 27/100\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.5583 - accuracy: 0.8356 - val_loss: 0.5317 - val_accuracy: 0.8904\n",
      "Epoch 28/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.5507 - accuracy: 0.8454 - val_loss: 0.5224 - val_accuracy: 0.8858\n",
      "Epoch 29/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.5429 - accuracy: 0.8415 - val_loss: 0.5148 - val_accuracy: 0.8767\n",
      "Epoch 30/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.5346 - accuracy: 0.8523 - val_loss: 0.5041 - val_accuracy: 0.8813\n",
      "Epoch 31/100\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.5266 - accuracy: 0.8483 - val_loss: 0.4949 - val_accuracy: 0.8767\n",
      "Epoch 32/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.5184 - accuracy: 0.8483 - val_loss: 0.4857 - val_accuracy: 0.8767\n",
      "Epoch 33/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.5100 - accuracy: 0.8483 - val_loss: 0.4765 - val_accuracy: 0.8767\n",
      "Epoch 34/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.5015 - accuracy: 0.8542 - val_loss: 0.4673 - val_accuracy: 0.8767\n",
      "Epoch 35/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.4930 - accuracy: 0.8542 - val_loss: 0.4593 - val_accuracy: 0.8813\n",
      "Epoch 36/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.4845 - accuracy: 0.8552 - val_loss: 0.4489 - val_accuracy: 0.8813\n",
      "Epoch 37/100\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.4763 - accuracy: 0.8562 - val_loss: 0.4415 - val_accuracy: 0.8858\n",
      "Epoch 38/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.4681 - accuracy: 0.8611 - val_loss: 0.4320 - val_accuracy: 0.8813\n",
      "Epoch 39/100\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.4604 - accuracy: 0.8601 - val_loss: 0.4235 - val_accuracy: 0.8858\n",
      "Epoch 40/100\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.4522 - accuracy: 0.8581 - val_loss: 0.4177 - val_accuracy: 0.8904\n",
      "Epoch 41/100\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.4445 - accuracy: 0.8679 - val_loss: 0.4055 - val_accuracy: 0.8813\n",
      "Epoch 42/100\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.4378 - accuracy: 0.8601 - val_loss: 0.4003 - val_accuracy: 0.8950\n",
      "Epoch 43/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.4304 - accuracy: 0.8620 - val_loss: 0.3938 - val_accuracy: 0.8858\n",
      "Epoch 44/100\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.4237 - accuracy: 0.8591 - val_loss: 0.3865 - val_accuracy: 0.8858\n",
      "Epoch 45/100\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.4170 - accuracy: 0.8620 - val_loss: 0.3764 - val_accuracy: 0.8950\n",
      "Epoch 46/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.4104 - accuracy: 0.8659 - val_loss: 0.3726 - val_accuracy: 0.8904\n",
      "Epoch 47/100\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.4049 - accuracy: 0.8650 - val_loss: 0.3644 - val_accuracy: 0.8904\n",
      "Epoch 48/100\n",
      "1022/1022 [==============================] - ETA: 0s - loss: 0.3940 - accuracy: 0.93 - 0s 25us/step - loss: 0.3987 - accuracy: 0.8708 - val_loss: 0.3562 - val_accuracy: 0.8904\n",
      "Epoch 49/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3932 - accuracy: 0.8650 - val_loss: 0.3526 - val_accuracy: 0.8904\n",
      "Epoch 50/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3882 - accuracy: 0.8699 - val_loss: 0.3472 - val_accuracy: 0.8950\n",
      "Epoch 51/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3829 - accuracy: 0.8699 - val_loss: 0.3421 - val_accuracy: 0.8904\n",
      "Epoch 52/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3780 - accuracy: 0.8728 - val_loss: 0.3364 - val_accuracy: 0.8904\n",
      "Epoch 53/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3736 - accuracy: 0.8689 - val_loss: 0.3305 - val_accuracy: 0.8904\n",
      "Epoch 54/100\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3687 - accuracy: 0.8738 - val_loss: 0.3292 - val_accuracy: 0.8904\n",
      "Epoch 55/100\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3651 - accuracy: 0.8650 - val_loss: 0.3238 - val_accuracy: 0.8904\n",
      "Epoch 56/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3611 - accuracy: 0.8728 - val_loss: 0.3191 - val_accuracy: 0.8904\n",
      "Epoch 57/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3573 - accuracy: 0.8757 - val_loss: 0.3142 - val_accuracy: 0.8904\n",
      "Epoch 58/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3534 - accuracy: 0.8708 - val_loss: 0.3098 - val_accuracy: 0.8904\n",
      "Epoch 59/100\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3502 - accuracy: 0.8728 - val_loss: 0.3079 - val_accuracy: 0.8904\n",
      "Epoch 60/100\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3471 - accuracy: 0.8718 - val_loss: 0.3035 - val_accuracy: 0.8904\n",
      "Epoch 61/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3438 - accuracy: 0.8738 - val_loss: 0.3010 - val_accuracy: 0.8904\n",
      "Epoch 62/100\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.3408 - accuracy: 0.8708 - val_loss: 0.2969 - val_accuracy: 0.8904\n",
      "Epoch 63/100\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3374 - accuracy: 0.8728 - val_loss: 0.2963 - val_accuracy: 0.9041\n",
      "Epoch 64/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3350 - accuracy: 0.8718 - val_loss: 0.2906 - val_accuracy: 0.8904\n",
      "Epoch 65/100\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3321 - accuracy: 0.8728 - val_loss: 0.2909 - val_accuracy: 0.9041\n",
      "Epoch 66/100\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3302 - accuracy: 0.8718 - val_loss: 0.2860 - val_accuracy: 0.8904\n",
      "Epoch 67/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3278 - accuracy: 0.8748 - val_loss: 0.2839 - val_accuracy: 0.8995\n",
      "Epoch 68/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3258 - accuracy: 0.8738 - val_loss: 0.2812 - val_accuracy: 0.8858\n",
      "Epoch 69/100\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3237 - accuracy: 0.8767 - val_loss: 0.2816 - val_accuracy: 0.9041\n",
      "Epoch 70/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3217 - accuracy: 0.8777 - val_loss: 0.2776 - val_accuracy: 0.9041\n",
      "Epoch 71/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3193 - accuracy: 0.8738 - val_loss: 0.2752 - val_accuracy: 0.8995\n",
      "Epoch 72/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3173 - accuracy: 0.8748 - val_loss: 0.2755 - val_accuracy: 0.9041\n",
      "Epoch 73/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3158 - accuracy: 0.8757 - val_loss: 0.2720 - val_accuracy: 0.9041\n",
      "Epoch 74/100\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3140 - accuracy: 0.8757 - val_loss: 0.2706 - val_accuracy: 0.9041\n",
      "Epoch 75/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3123 - accuracy: 0.8787 - val_loss: 0.2685 - val_accuracy: 0.9087\n",
      "Epoch 76/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3109 - accuracy: 0.8826 - val_loss: 0.2688 - val_accuracy: 0.9041\n",
      "Epoch 77/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3095 - accuracy: 0.8806 - val_loss: 0.2668 - val_accuracy: 0.9041\n",
      "Epoch 78/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3080 - accuracy: 0.8787 - val_loss: 0.2643 - val_accuracy: 0.9087\n",
      "Epoch 79/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3062 - accuracy: 0.8787 - val_loss: 0.2638 - val_accuracy: 0.9041\n",
      "Epoch 80/100\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3051 - accuracy: 0.8806 - val_loss: 0.2628 - val_accuracy: 0.9041\n",
      "Epoch 81/100\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3037 - accuracy: 0.8767 - val_loss: 0.2605 - val_accuracy: 0.9087\n",
      "Epoch 82/100\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3027 - accuracy: 0.8777 - val_loss: 0.2596 - val_accuracy: 0.9087\n",
      "Epoch 83/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3010 - accuracy: 0.8816 - val_loss: 0.2594 - val_accuracy: 0.9087\n",
      "Epoch 84/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3003 - accuracy: 0.8845 - val_loss: 0.2574 - val_accuracy: 0.9087\n",
      "Epoch 85/100\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2984 - accuracy: 0.8787 - val_loss: 0.2583 - val_accuracy: 0.9041\n",
      "Epoch 86/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2988 - accuracy: 0.8836 - val_loss: 0.2562 - val_accuracy: 0.9087\n",
      "Epoch 87/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2974 - accuracy: 0.8826 - val_loss: 0.2549 - val_accuracy: 0.9087\n",
      "Epoch 88/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2959 - accuracy: 0.8845 - val_loss: 0.2542 - val_accuracy: 0.9087\n",
      "Epoch 89/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2953 - accuracy: 0.8855 - val_loss: 0.2535 - val_accuracy: 0.9087\n",
      "Epoch 90/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2934 - accuracy: 0.8826 - val_loss: 0.2521 - val_accuracy: 0.9087\n",
      "Epoch 91/100\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2918 - accuracy: 0.8787 - val_loss: 0.2542 - val_accuracy: 0.9041\n",
      "Epoch 92/100\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2920 - accuracy: 0.8845 - val_loss: 0.2525 - val_accuracy: 0.9087\n",
      "Epoch 93/100\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2911 - accuracy: 0.8865 - val_loss: 0.2499 - val_accuracy: 0.9087\n",
      "Epoch 94/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2895 - accuracy: 0.8855 - val_loss: 0.2494 - val_accuracy: 0.9087\n",
      "Epoch 95/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2884 - accuracy: 0.8836 - val_loss: 0.2499 - val_accuracy: 0.9087\n",
      "Epoch 96/100\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2879 - accuracy: 0.8826 - val_loss: 0.2485 - val_accuracy: 0.9087\n",
      "Epoch 97/100\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2870 - accuracy: 0.8855 - val_loss: 0.2475 - val_accuracy: 0.9132\n",
      "Epoch 98/100\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.2871 - accuracy: 0.8845 - val_loss: 0.2482 - val_accuracy: 0.9087\n",
      "Epoch 99/100\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2844 - accuracy: 0.8865 - val_loss: 0.2472 - val_accuracy: 0.9178\n",
      "Epoch 100/100\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2844 - accuracy: 0.8875 - val_loss: 0.2465 - val_accuracy: 0.9087\n"
     ]
    }
   ],
   "source": [
    "# in python (rown, comlumns)\n",
    "\n",
    "import tensorflow as tf \n",
    "import pandas as pd #Importing panda allow us to fetch csv\n",
    "#import numpy as np \n",
    "#from tensorflow.python.ops  import *\n",
    "import csv\n",
    "import os\n",
    "#Importicontrol /name Microsoft.DeviceManager ng csv from file into a df variable\n",
    "df=pd.read_csv('housepricedata.csv')\n",
    "dataset = df.values # convert df into arrays for our machine to process:\n",
    "#split our dataset into input features (X) and the feature we wish to predict (Y). To do that split, we simply assign \n",
    "#the first 10 columns of our array to a variable called X and the last column of our array to a variable called Y.\n",
    "X = dataset[:,0:10]#we take our dataset and make the first 10 columns x variables; 10 doesnt count, only 1 to 9 columns\n",
    "Y = dataset[:,10]#We  assign the last column of our array to Y:\n",
    "\n",
    "#we need to normalize the data so the scale is not so different between columns\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler =  preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)#min-max scaler scales the dataset \n",
    "#so that all the input features lie between 0 and 1 inclusive:\n",
    "\n",
    "from sklearn.model_selection import train_test_split#split our dataset into a training set and a test set\n",
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)#This tells scikit-learn that \n",
    "#your val_and_test size will be 30% of the overall dataset\n",
    "\n",
    "#Unfortunately, this function only helps us split our dataset into two. Since we want \n",
    "#a separate validation set and test set, we can use the same function to do the split again on val_and_test:\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "\n",
    "#Importing the keras libraries so we can use neural networks\n",
    "import keras #to import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#OUr model is 4 layers (2 hidden), 1 input, 1 output (input layer has 10 neurons, 1st and 2nd hidden layers have 32 neurons each, output layer has 1 neuron)\n",
    "#Hidden layer 1: 32 neurons, ReLU activation\n",
    "#Hidden layer 2: 32 neurons, ReLU activation\n",
    "#Output Layer: 1 neuron, Sigmoid activation\n",
    "\n",
    "#model = Sequential([ ... ])This says that we will store our model in the variable ‘model’, \n",
    "#and we’ll describe it sequentially (layer by layer) in between the square brackets.\n",
    "# first layer as a dense layer with 32 neurons, ReLU activation and the input shape is 10 since we have 10 input features. \n",
    "#Note that ‘Dense’ refers to a fully-connected layer\n",
    "#second layer is also a dense layer with 32 neurons, ReLU activation: Dense(1, activation='sigmoid'),\n",
    "#do not have to describe the input shape since Keras can infer from the output of our first layer.\n",
    "#third layer is a dense layer with 1 neuron, sigmoid activation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential([Dense(32, activation='relu', input_shape=(10,)),Dense(32, activation='relu'),Dense(1, activation='sigmoid'),])\n",
    "\n",
    "#Now we choose the algorithms:optimizer='sgd'‘sgd’ refers to stochastic gradient descent\n",
    "#loss function for outputs that take the values 1 or 0 is called binary cross entropy: loss='binary_crossentropy'\n",
    "#to track accuracy on top of the loss function: metrics=['accuracy']\n",
    "\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#The function is called ‘fit’ as we are fitting the parameters to the data. \n",
    "#We have to specify what data we are training on, which is X_train and Y_train. \n",
    "#Then, we specify the size of our mini-batch and how long we want to train it for (epochs). \n",
    "#Lastly, we specify what our validation data is \n",
    "#so that the model will tell us how we are doing on the validation data at each point.\n",
    "#This function will output a history, which we save under the variable hist.\n",
    "\n",
    "hist = model.fit(X_train, Y_train,batch_size=32, epochs=100,validation_data=(X_val, Y_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02398077],\n",
       "       [0.02398077],\n",
       "       [0.02398077],\n",
       "       [0.02398077],\n",
       "       [0.02398077],\n",
       "       [0.02398077],\n",
       "       [0.02398077],\n",
       "       [0.02398077],\n",
       "       [0.02398079],\n",
       "       [0.02398079]], dtype=float32)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new instance where we do not know the answer\n",
    "\n",
    "\n",
    "#X_new_no_reshape=np.array([0.1,0.3,0.4,0.7,0.8,0.9,0.1,0.8,0.9,0.077])\n",
    "X_new_no_reshape=np.array([18246,5,8,1060,1,0,3,6,1,270])\n",
    "\n",
    "X_new=X_new_no_reshape.reshape(1,10);#Reshaphing the array so it's not (10,) which uses a buffer index\n",
    "X_new=0.01*np.ones((10,10))\n",
    "#X_new=np.array([[0.1],[0.3],[0.4],[0.7],[0.8],[0.9],[0.1],[0.8],[0.9]])\n",
    "\n",
    "# make a prediction\n",
    "ynew = model.predict(X_new)\n",
    "# show the inputs and predicted outputs\n",
    "#print(\"X=%s, Predicted=%s\" % (Xnew[0], ynew[0]))\n",
    "ynew\n",
    "#(model.predict(X_new) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
